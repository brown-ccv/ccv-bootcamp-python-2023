{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Walkthrough\n",
    "Today we will be demonstrating the following key exploratory data analysis techniques using an example dataset:\n",
    "\n",
    "**Agenda:**\n",
    "1. Importing libraries & packages\n",
    "2. Importing tabular data to a DataFrame\n",
    "3. Inspecting DataFrame structure\n",
    "4. Concatenation\n",
    "5. Renaming Columns\n",
    "6. Exploring values\n",
    "7. Exporting DataFrames\n",
    "8. Merging\n",
    "9. Plotting\n",
    "\n",
    "\n",
    "\n",
    "## The Data\n",
    "Our example dataset is daily summaries of air quality data from Providence, RI. It will give you some experience with working with temporal data.\n",
    "\n",
    "The Rhode Island Department of Environmental Management (RIDEM) and Rhode Island Department of Health (RIDOH) collects air quality data at several sites across Rhode Island. We will be examining data from one site at the Community of Rhode Island (CCRI) Liston Campus. Here's some background:\n",
    "\n",
    "* The CCRI site is part of the EPA's *State or Local Air Monitoring Stations* (SLAMS) and *National Air Toxics Trends Sites* (NATTS) networks.\n",
    "* A variety of air pollutants (particulate matter (PM), volatile organic carbon (VOCs),  polycyclic aromatic hydrocarbons (PAHs), carbonyls, black carbon) have been monitored at this site since 2005.\n",
    "* A reference for some of the dataset's [field descriptions](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_daily_summary_files).\n",
    "* The data was obtained from the Environmental Protection Agency (EPA) [Air Quality Data website](https://www.epa.gov/outdoor-air-quality-data).\n",
    "        <div>\n",
    "        <img src=\"./images/aq-site-info.png\" width=\"400\"/>\n",
    "        </div>\n",
    "        <div>\n",
    "        <img src=\"./images/aq-site-gearth-dist.png\" width=\"700\"/>\n",
    "        </div>\n",
    "\n",
    "We will use a subset of this data in the demonstrations below and give you a chance to work with a larger dataset during the hands-on lab.\n",
    "\n",
    "*Links*\n",
    "[EPA Air Quality Data Interactive Map](https://www.epa.gov/outdoor-air-quality-data/interactive-map-air-quality-monitors) - Data source\n",
    "[RIDEM 2022 Annual Monitoring Report](https://dem.ri.gov/sites/g/files/xkgbur861/files/2023-01/airnet22.pdf) - More information about the site and other monitoring locations across the state.\n",
    "[National Air Toxics Tends Sites Quality Assurance Project Plan](https://www3.epa.gov/ttnamti1/files/ambient/airtox/NATTS-UATMP-PAMS-SNOC-Analytical-Support-QAPP-2019.pdf) - Detailed measurement guidelines for the toxins in this dataset\n",
    "[National Ambient Air Quality Standards](https://www.epa.gov/criteria-air-pollutants/naaqs-table)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries & packages\n",
    "Importing packages typically appears at the top of the file.\n",
    "* `import <package_name>` is the most basic command\n",
    "* The package can be imported with an alias to shorten verbosity. Common packages will often have a conventional alias.\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "pandas.read_csv('path')\n",
    "\n",
    "# VS as an alias\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_csv('path')\n",
    "```\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas library as an alias of 'pd'\n",
    "import matplotlib.pyplot as plt  # Import the sub-package pyplot from the matplotlib library as an alias of 'plt'\n",
    "from pathlib import Path  # Import filesystem path package, for easier pathing to files and outputs\n",
    "import os  # Library with operating system functions like listing directories\n",
    "\n",
    "# Magic command for jupyter notebook to generate figures within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing tabular data to a DataFrame\n",
    "The pandas package reads tabular data into a data structure called a `DataFrame`. Some examples of read functions are below:\n",
    "* [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) - Comma-delimited or other delimited files\n",
    "* [`pd.read_fwf`](https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html#) - Fixed width files\n",
    "* [`pd.read_excel`](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) - Microsoft excel files\n",
    "* [`pd.read_sql`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) - SQL query or database table\n",
    "* See [pandas I/O documentation](https://pandas.pydata.org/docs/reference/io.html#input-output) for more examples\n",
    "\n",
    "We will be working with the `pd.read_csv()` because our data is comma-delimited. This function defaults to read comma-delimited files, but can be used on any delimited text file when the seperator is specified.\n",
    "\n",
    "A. To start we need specify the path to our data directory:\n",
    "```\n",
    "project\n",
    "├── data\n",
    "│   └── raw\n",
    "│       └── monthly   <- Data is here\n",
    "│\n",
    "└── notebooks         <- Our working directory is here\n",
    "```\n",
    "We will be using package `os` and `Path` from `pathlib` to create out directory path because it standardizes pathing between operating systems. Path separators are different between Unix (Mac & Linux; using `/`) and Windows (using `\\`) operating systems. Avoiding full string paths makes the code universal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to the data directory. The `..` means go up one level from the current working directory\n",
    "data_path = Path('..', 'data')\n",
    "\n",
    "## We extend the path to the monthly data directory\n",
    "path_to_monthly_data = data_path / 'raw' / 'monthly'\n",
    "path_to_monthly_data2 = data_path.joinpath('raw',\n",
    "                                           'monthly')  # Alternative syntax for extending path\n",
    "\n",
    "print(f'This is the monthly data directory: {path_to_monthly_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the path generated above, we will read the first month of data (January 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and save the DataFrame object to a variable 'df_2022_01'\n",
    "df_2022_01 = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting DataFrame Structure\n",
    "Now that we have imported the data to a DataFrame. Some questions we are curious about:\n",
    "1. Did it import correctly?\n",
    "2. What does the table look like? Number of rows? Columns?\n",
    "3. Do we need all the data we are importing?\n",
    "4. Is the data in the correct format?\n",
    "\n",
    "We can inspect the DataFrame object by looking at its **attributes** and using DataFrame **methods**.\n",
    "\n",
    "Here are useful **attributes** of the dataframe\n",
    "* `.shape`:  Table dimensions\n",
    "* `.columns`:  Sequence of columns\n",
    "* `.index`:  Sequence of row indexes/labels\n",
    "* `.dtypes`: Data types by column\n",
    "\n",
    "Here are a few useful **methods** to inspect a dataframe:\n",
    "* `.head()`: Shows the first 5 rows--can change the number by supplying an integer.\n",
    "* `.tail()`: Shows the last 5 rows--can change the number by supplying an integer.\n",
    "* `.info()`: Combines several DataFrame attributes to one report.\n",
    "* `.select_dtypes()`: Useful for viewing only columns of certain data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Python objects may have <b>attributes</b> and <b>methods</b>.\n",
    "\n",
    "<b>attributes</b> - Are properties of the object type. Say that there is a `Person` object, the person's favorite food is one of their properties.\n",
    "<b>methods</b> - Are functions bound to an object type. They often perform a process that uses the object's properties. A method of a `Person` object, could be report writing.\n",
    "\n",
    "Attributes and methods are accessed by using dot (`.`) connectors to the object. The difference is that methods have `()` at the end so arguments can be passed.\n",
    ">Example:\n",
    "```\n",
    "George.favorite_food  # Accessing an attribute\n",
    ">>> 'Pho'\n",
    "my_report = George.write_report(topic='favorite_food', pages=5)  # Calling a method\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "What is an \"object\" dtype?\n",
    "\n",
    "<b>Short Answer:</b> It is a column of string or mixed data types (e.g. string, ints, floats, etc). Typically object dtype columns from an imported CSV will be a column of strings.\n",
    "\n",
    "<b>Long Answer:</b>  Pandas was built upon the numpy package on its backend. Numpy can only store information in an array where each value is encoded in the same number of bytes. Because strings can be of variable length, they do not conform to the fixed byte requirement. Instead Pandas creates an object array with pointers to the strings and  the pointers are of equal byte size. This is similar for columns with mixtures of data types.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Numerical Fields\n",
    "df_2022_01.select_dtypes(include=['int', 'float']).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Object fields\n",
    "df_2022_01.select_dtypes(include='object').head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to our questions:**\n",
    "\n",
    "1. Did it import correctly?\n",
    "2. What does the table look like? Number of rows? Columns?\n",
    "3. Do we need all the data we are importing?\n",
    "4. Is the data in the correct format?\n",
    "\n",
    "* There are many columns we could drop because they all have the same value such as: \"Local Site Name\" and \"Address\". We know we are only working with one site for this analysis so these columns don't provide much value. These columns are long string fields that take up more memory. Dropping them would improve performance if this dataset gets really large.\n",
    "* The date would be more useful as a datetime data type rather than as string. This will allow for filtering by time and other useful datetime operations.\n",
    "\n",
    "We can supply additional arguments to the `read_csv` function to handle these specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the columns we wish to keep\n",
    "keep_cols = ['Parameter Code', 'POC', 'Parameter Name', 'Duration Description',\n",
    "             'Pollutant Standard',\n",
    "             'Date (Local)', 'Year', 'Day In Year (Local)', 'Units of Measure',\n",
    "             'Exceptional Data Type',\n",
    "             'Observation Count', 'Observation Percent', 'Arithmetic Mean', 'First Maximum Value',\n",
    "             'First Maximum Hour', 'AQI', 'Daily Criteria Indicator', ]\n",
    "\n",
    "# Read in the csv with additional arguments\n",
    "df_2022_01_curated = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_01.csv',\n",
    "                                 usecols=keep_cols,  # Specify columns to keep\n",
    "                                 parse_dates=['Date (Local)'],\n",
    "                                 # Specify column to parse as a date instead of string\n",
    "                                 date_format='%Y-%m-%d',  # Specify the format of date strings\n",
    "                                 )\n",
    "df_2022_01_curated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01_curated.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've cut down the number of columns and converted the date field to a datetime format!\n",
    "Next lets see how we can add more data from other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concatenation\n",
    "So far we've worked with one month's worth of data. Let's see how we can combine DataFrames together.\n",
    "\n",
    "We will be using the [`pd.concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) function to combine two or more DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in February data\n",
    "df_2022_02_curated = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_02.csv',\n",
    "                                 usecols=keep_cols,  # Specify columns to keep\n",
    "                                 parse_dates=['Date (Local)'],\n",
    "                                 # Specify column to parse as a date instead of string\n",
    "                                 date_format='%Y-%m-%d',  # Specify the format of date strings\n",
    "                                 )\n",
    "list_df_to_concat = [df_2022_01_curated, df_2022_02_curated]\n",
    "df_combined = pd.concat(list_df_to_concat)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "After we concat we need to remember to reset the index because it does not do this automatically! We use the method `reset_index()` to do this.\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Notice how the index is not the length of the dataframe?\n",
    "df_combined.index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Resetting index\n",
    "df_combined = df_combined.reset_index(drop=True)\n",
    "df_combined.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat all the files!\n",
    "Now that we've learned how to concatenate files. Let's combine all the monthly data.\n",
    "Doing it manually for each file would be cumbersome. So lets use a function!\n",
    "\n",
    "We won't have time to walk through this function in detail, but we encourage you to take a look on your own time. It covers concepts of:\n",
    "1) Defining a function with: arguments, defaults, and variable keyword arguments\n",
    "2) Listing files in a directory\n",
    "3) for loops\n",
    "4) if/else constructs\n",
    "5) What we just learned about reading csv and concatenation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell defines the function below. Then we can call and reuse it in future cells.\n",
    "\n",
    "def combine_csv_files(path, prefix, suffix='.csv', **kwargs):\n",
    "    \"\"\"\n",
    "    Searches a directory for text files, imports as pandas.DataFrames and\n",
    "    concatenates to a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        path: pathlib.Path object of a directory\n",
    "        prefix: String prefix to search for\n",
    "        suffix: String suffix to search for, Optional, Default '.csv'\n",
    "        **kwargs: Variable additional arguments pass to the pandas.read_csv function\n",
    "\n",
    "    Returns: pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    # List files in the directory\n",
    "    list_files_in_path = sorted(os.listdir(path))\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    list_df = []\n",
    "\n",
    "    # Loop through files\n",
    "    for file in list_files_in_path:\n",
    "        # Check if the file starts with the prefix and ends with the suffix\n",
    "        if file.startswith(prefix) and file.endswith(suffix):\n",
    "            # Read in csv as DataFrame and append it to the list\n",
    "            list_df.append(pd.read_csv(path / file, **kwargs))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Concatenate the DataFrames\n",
    "    df_return = pd.concat(list_df).reset_index(drop=True)\n",
    "\n",
    "    # Return the concatenated DataFrame\n",
    "    return df_return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function defined above\n",
    "df_2022 = combine_csv_files(path_to_monthly_data,\n",
    "                            prefix='daily_44_007_0022_2022_', # Read in files only starting with this prefix\n",
    "                            suffix='.csv',  # Read in files only ending with this suffix\n",
    "                            usecols=keep_cols,  # Specify columns to keep\n",
    "                            parse_dates=['Date (Local)'], # Specify column to parse as a date instead of string\n",
    "                            date_format='%Y-%m-%d', )  # Specify the format of date strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Renaming Columns\n",
    "Before we jump into more detailed EDA we'll want to update the DataFrame to make our lives a bit easier. We'll be typing column names often to query the data so let's start by renaming the columns to a standard format.\n",
    "\n",
    "1. lowercase\n",
    "2. underscores instead of spaces\n",
    "3. simplify complex names\n",
    "\n",
    "Methods we'll use:\n",
    "1. [`str.lower()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html): Lowercases the column names\n",
    "2. [`str.replace()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html): Finds and replaces sub-strings\n",
    "3. [`rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html): Renames columns or index labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we access columns with the columns attribute\n",
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can cast a lowercase method across the columns with the `str.lower()` method\n",
    "df_2022.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cast a `str.replace()` method after the lowercase method. Though this starts to look hard to read.\n",
    "df_2022.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# A cleaner way is to wrap it in (), allowing us to separate the dot connectors to multiple lines\n",
    "(df_2022.columns\n",
    " .str.lower()  # Lowercase the names\n",
    " .str.replace(' ', '_')  # Replace Spaces with underscores\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above examples are output demonstrations. It doesn't actually change the dataframe's column until we assign the output.\n",
    "print('---Before Assignment---')\n",
    "print(df_2022.columns)\n",
    "\n",
    "# Assigning reformatted column names to the DataFrame's columns attribute\n",
    "df_2022.columns = (df_2022.columns\n",
    "                   .str.lower()  # Lowercase the names\n",
    "                   .str.replace(' ', '_')  # Replace Spaces with underscores\n",
    "                   )\n",
    "print('---After Assignment---')\n",
    "print(df_2022.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the rename method to rename specific columns with complex symbols\n",
    "# We supply a dictionary mapping the column names we want to change as the key and the new name as the value.\n",
    "df_2022 = df_2022.rename(columns={'date_(local)': 'date',\n",
    "                                  'day_in_year_(local)': 'day_in_year',\n",
    "                                  }\n",
    "                         )\n",
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! Now are column names are much more manageable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Exploring Values\n",
    "Let's start exploring the dataset.\n",
    "\n",
    "Questions:\n",
    "1. How complete is the data?\n",
    "2. Are there duplicates?\n",
    "3. How many parameters are measured?\n",
    "4. How often are the parameters measured?\n",
    "5. What are the descriptive stats of the numeral data?\n",
    "\n",
    "We will cover the following topics:\n",
    "1. Indexing\n",
    "2. Checking for Nulls and Duplicates\n",
    "3. Counts and Uniques\n",
    "4. Querying\n",
    "5. Descriptive Stats and groupby"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1 Indexing\n",
    "We often will need to look different slices of the dataframe.\n",
    "* Slices are different groups of rows and/or columns.\n",
    "* We perform indexing using brackets `[]`, similar to lists.\n",
    "\n",
    "```\n",
    "# For a single column\n",
    "df_2022['parameter_name']\n",
    "\n",
    "# For multiple columns\n",
    "df_2022[['parameter_name', 'parameter_code']]\n",
    "\n",
    "# For a slice of rows (same as list indexing)\n",
    "df_2022[5:10]\n",
    "\n",
    "# For a slice of rows and columns use the .loc method\n",
    "df_2022.loc[5:10, ['parameter_name', 'parameter_code']]\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022.loc[5:10, ['parameter_name', 'parameter_code']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2 Checking for Nulls\n",
    "Nulls or NAs are values that represent no data or missing data.\n",
    "* They are often represented as **NaN** for \"not a number\"\n",
    "* Pandas will typically fill NaNs for blank values upon import.\n",
    "* Be aware that scientific datasets often use large negative numbers outside of the normal range (like -999) to represent null data.\n",
    "\n",
    "We can check for the number of null values quickly using the method `isna()` and summing the results\n",
    "* `isna()` will create a boolean matrix and `sum()` will sum by columns.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luckily this dataset does not have missing data in important fields. The fields with null values make sense.\n",
    "* Not all chemicals measured have a regulatory standard that would be in the *pollutant_standard* field, only the most toxic.\n",
    "* *exceptional_data_type* is a flag field for anomalous conditions and events.\n",
    "\n",
    "Pandas has great documentation for working with and filling in missing data that we recommend reviewing. [Link](https://pandas.pydata.org/docs/user_guide/missing_data.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.3 Checking for Duplicates\n",
    "We use the [`duplicated()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html) method to check for duplicates.\n",
    "\n",
    "The `duplicated()` method generates a boolean array of indicating the duplicates based on a subset of keys.\n",
    "\n",
    "Our intuition is that this dataset should have one record per day for each parameter. Let's check if that assumption is correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a boolean array of duplicates\n",
    "mask_duplicates = df_2022.duplicated(subset=['date', 'parameter_code'], keep=False)\n",
    "mask_duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We supplied duplicated method with the `date` and `parameter_code` as the subset of columns to act as a unique key.\n",
    "* We also supply a keyword `keep=False`.\n",
    "    * This logic is very confusing.\n",
    "        * The default behavior of this method is to mark only duplicates after the first occurrence as the duplicates.\n",
    "        * This makes easy \"keep\" the first occurrence and remove all subsequent duplicates.\n",
    "        * By supplying `False` we are turning off that behavior because we want to consider all duplicates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_duplicates.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK. There are clearly some duplicates in this dataset. Let's take a closer look at these in the next section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.4 Querying and Sorting\n",
    "By querying the dataframe we can better examine the duplicates we identified in the previous section.\n",
    "\n",
    "There are two main ways to query a DataFrame:\n",
    "1. Masking and Indexing\n",
    "2. [`query()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html) method\n",
    "\n",
    "The choice depends on preference and context.\n",
    "* Masking and Indxing is clearer for complex queries with multiple conditions.\n",
    "* The `query()` method good for short and quick queries.\n",
    "\n",
    "Syntax example:\n",
    "```Python\n",
    "# Masking and Indexing\n",
    "mask_param = df_2022['parameter_name'] == 'PM2.5 - Local Conditions'\n",
    "df_2022[mask_param]\n",
    "\n",
    "# Query method\n",
    "df_2022.query('parameter_name == \"PM2.5 - Local Conditions\"')\n",
    "```\n",
    "\n",
    "Conveniently we just created a mask of duplicate values in the last section so let's examine that.\n",
    "We will also use the [`sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) method to order the DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Masking and Indexing\n",
    "df_2022[mask_duplicates].sort_values(by=['date', 'parameter_name'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The PM2.5 measurment seems to have something different going on than the other duplicates. Let's take a look at that parameter first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_2022.loc[mask_duplicates, ['date', 'parameter_name', 'duration_description', 'pollutant_standard', 'arithmetic_mean', 'first_maximum_value', 'first_maximum_hour','aqi']]\n",
    " .sort_values(by=['date', 'parameter_name', 'duration_description', 'pollutant_standard'])\n",
    " .head(12))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*What do we notice about the PM2.5 measurement?*\n",
    "* There are 3 records per day. They differ in duration_description and pollutant_standard.\n",
    "* The \"1 HOUR\" duration seems to have the most accurate information for our purposes because has more \"arithmetic_mean\" precision, and better information about the first maximum value and hour over the day.\n",
    "* The \"1 Hour\" duration records does not have Air Quality Index \"aqi\" information\n",
    "\n",
    "**Conclusion**\n",
    "* We should keep only the \"1 HOUR\" duration and remove the others.\n",
    "* We'll save the other durations in a separate DataFrame in the case we want the daily AQI information.\n",
    "\n",
    "We can do this with querying!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a mask for the values we want to remove\n",
    "mask_remove = (df_2022['parameter_name'] == 'PM2.5 - Local Conditions') & (df_2022['duration_description'] != '1 HOUR')\n",
    "\n",
    "# Create dataframe of PM2.5 records we remove\n",
    "df_removed_records = df_2022[mask_remove].reset_index(drop=True)\n",
    "\n",
    "# Create dataframe of everything we want to keep. The NOT (~) operator reverses the mask.\n",
    "df_2022_cleaned_v1 = df_2022[~mask_remove].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_cleaned_v1.query(\"parameter_name == 'PM2.5 - Local Conditions'\").head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's return to look at those other duplicate parameters. Below we will combine two masks. Note that we are referencing the pre-cleaned dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is an example of combining two masks. The duplicate mask and the not PM2.5 mask\n",
    "mask_not_pm2p5 = df_2022['parameter_name'] != 'PM2.5 - Local Conditions'\n",
    "df_2022[mask_duplicates & mask_not_pm2p5].sort_values(by=['date', 'parameter_name'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*What do we notice here?*\n",
    "* It appears these duplicates are pairs with a different 'poc' code.\n",
    "    * The 'poc' code is an identifier for sampling instruments at the site. These parameters are sampled in replicates on separate instruments as a quality-control check. If replicate results differ too much, it may signal sampling error.\n",
    "* At this point we have several options:\n",
    "    1. Drop one of the duplicates\n",
    "    2. Average the duplicates\n",
    "    3. Leave the data alone\n",
    "\n",
    "We'll choose option 3 for now because the duplicate treatment may depend on the type of analysis being performed. The decision can be made at that point."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.5 Counts and Uniques\n",
    "Counts and unique values are great for looking at categorical data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the count of unique values in each column\n",
    "df_2022_cleaned_v1.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a list of unique values in a column\n",
    "df_2022_cleaned_v1['parameter_name'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the counts of each parameter\n",
    "counts = df_2022_cleaned_v1['parameter_name'].value_counts()\n",
    "counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is abbreviated. To see specific analytes you need index with a list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts[['PM2.5 - Local Conditions', 'Lead PM10 STP', 'Acetaldehyde', 'Cyclohexane']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.5 Descriptive Stats and groupby\n",
    "The method `.describe()` is a great way to get descriptive statistics on numerical columns. When called on the dataframe, it will automatically provide statistics on each numerical column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_cleaned_v1.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Though this isn't very useful for our dataset at the moment because it's creating statistics across 90+ parameters. It would be better to chunk the DataFrame by parameter and run describe. This is where a powerful method called `groupby` is useful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a Groupby object with parameter_name\n",
    "# This object chunks the dataframe by parameter_name\n",
    "gb_param = df_2022_cleaned_v1.groupby(by='parameter_name')\n",
    "gb_param['arithmetic_mean'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To look at the PM2.5 stats\n",
    "df_gb_stats = gb_param['arithmetic_mean'].describe()\n",
    "df_gb_stats.loc['PM2.5 - Local Conditions']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can groupby multiple columns. Here's an example of sample counts by parameter and month."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We group by parameter name and the data column converted to the month number\n",
    "gb_param_month = df_2022_cleaned_v1.groupby(by=['parameter_name', df_2022_cleaned_v1['date'].dt.month])\n",
    "df_param_monthly_counts = gb_param_month['arithmetic_mean'].count()  # Count the number of records\n",
    "df_param_monthly_counts.loc[['PM2.5 - Local Conditions', '1,1,1,2-Tetrachloroethane']]  # View two specific parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the frequency of sampling between these parameters is different."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Previously we found that some parameters had duplicate values. We can use groupby to average and count the values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gb_param_date = df_2022_cleaned_v1.groupby(by=['parameter_name', 'date'])\n",
    "(gb_param_date['arithmetic_mean']\n",
    " .agg(['mean', 'count'])  # We use the aggregate method to apply both 'mean' and 'count'\n",
    " .query(\"count > 1\")  # Query for only those with more than one record\n",
    " )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Exporting DataFrames\n",
    "We made some interesting groupby tables with summary stats in the last section. Let's try exporting them so we can view in another application or import them in a future analysis.\n",
    "\n",
    "We use the `to_csv()` method to export to csv.\n",
    "See the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) to see all the other formats you can export to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a Path object to the reports folder\n",
    "outputs_path = Path('..', 'reports')\n",
    "\n",
    "# Save the groupby parameters and descriptive stats of the arithmetic mean to a DataFrame\n",
    "df_gb_param_mean_describe = gb_param['arithmetic_mean'].describe()\n",
    "\n",
    "# Call the to_csv method on the dataframe specifying path and filename\n",
    "df_gb_param_mean_describe.to_csv(outputs_path / 'stats-arithmetic_mean-by-param.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Navigate to the reports directory and confirm that a file was written."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Merging\n",
    "If we wanted to join two tables based on common key we would use the [`merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) method.\n",
    "\n",
    "There is a file with parameter classes that categorizes the parameters. This might help us make sense of which parameters are related to each other. Let's join it to our dataframe!\n",
    "```\n",
    "project\n",
    "├── data\n",
    "│   └── raw\n",
    "│       └── params_class.csv\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_param_classes = pd.read_csv(data_path / 'raw' / 'params_class.csv')\n",
    "df_param_classes.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This file has two new columns:\n",
    "1. **param_class** - The parameter classification\n",
    "2. **tier1** - A boolean field for EPA designated Tier 1 major risk toxins.\n",
    "    * Note that this does not apply to particulate matter which are criteria pollutants but are not designated as toxins."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count of the number of parameters in each class\n",
    "df_param_classes['param_class'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query the 18 Tier 1 toxins\n",
    "df_param_classes.query(\"tier1 == True\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge the dataframes using parameter_code as the key with a left-join\n",
    "df_2022_cleaned_v2 = df_2022_cleaned_v1.merge(df_param_classes, on='parameter_code', how='left')\n",
    "df_2022_cleaned_v2.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We used a left-join here. This means that only keys from the left table matter in the join. Keys that are in the right table but not the left will not be joined.\n",
    "\n",
    "Notice that parameter_name is duplicated with an appended \"_x\" and \"_y\" because the column existed in both tables. You can prevent this by indexing only the columns you want to join. For now we are just going to drop and rename the columns for an example of the `drop()` method!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_cleaned_v3 = (df_2022_cleaned_v2\n",
    "                      .drop(columns='parameter_name_y')  # Remove the column\n",
    "                      .rename(columns={'parameter_name_x': 'parameter_name'})  # We've seen this method previously!\n",
    "                      )\n",
    "df_2022_cleaned_v3.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! Lets save a copy of our processed dataset to the processed directory of the data folder.\n",
    "This is good practice so that future analysis can start from a cleaner version of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_cleaned_v3.to_csv(data_path / 'processed' / 'df_2022_processed.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Reshaping and Plotting\n",
    "\n",
    "We will be plotting with Pandas' built-in plotting methods and functions.\n",
    "* Pandas plotting is built as a front-end to the Matplotlib package.\n",
    "* The plotting functionalities are bare-bones but are good for quick EDA.\n",
    "* There are many other plotting packages out there here are a few:\n",
    "    * [matplotlib](https://matplotlib.org) - The cornerstone of several plotting libraries, including pandas. It's hard to learn but necessary for fine-tuning figures.\n",
    "    * [seaborn](https://seaborn.pydata.org) - Based on matplotlib but with a higher-level API easier to start with.\n",
    "    * [plotnine](https://plotnine.readthedocs.io/en/stable/) - Implementation of R's popular ggplot2 in python. Also based matplotlib.\n",
    "    * [vega-altair](https://altair-viz.github.io) - Declarative graphing library built upon vega-lite grammar. Great for those with JavaScript background.\n",
    "    * [plotly](https://plotly.com/python/) - For interactive graphics\n",
    "    * [bokeh](https://bokeh.org) -Also for interactive graphics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reshaping\n",
    "Plotting will require many dataframe transformations to shape the dataframe into a plot-able structure.\n",
    "* With timeseries data we typically want a structure where rows are a single date/time and plot values are along columns.\n",
    "\n",
    "    | Date | param1 | param_2 | ...   | param_n |\n",
    "    |-----|-------|-------|-------|-------|\n",
    "    | Date1 | value | value | value | value |\n",
    "    | Date2 | value | value | value | value |\n",
    "    | ... | value | value | value | value |\n",
    "\n",
    "The [`pd.pivot_table()`](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) function will help us reshape the table to this format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will only look at particulate matter for plotting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_c = df_2022_cleaned_v3 # saving as a shorter name for convenience\n",
    "\n",
    "mask_pm = (df_2022_c['param_class'] == 'particulate')  # mask for just the particulate parameters\n",
    "df_pm = pd.pivot_table(df_2022_c[mask_pm],\n",
    "                       values='arithmetic_mean',  # The column we want as values\n",
    "                       index='date',  # Set the date as the index of the new dataframe\n",
    "                       columns='parameter_name'  # The column with labels we wish to appear as columns\n",
    "                       )\n",
    "df_pm.columns = ['Black Carbon', 'PM10', 'PM2.5', 'Particle Number']  # Rename columns for convenience\n",
    "df_pm.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a wide table with the index as datas and parameters as columns and can begin plotting!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting\n",
    "1. Boxplots - Distribution and outliers\n",
    "2. Time Series - Temporal trends\n",
    "3. Scatter - Relationships between variables\n",
    "4. Autocorrelation & Lag Plots - Relationships with previous measurements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.1 Boxplots\n",
    "Boxplots can be generated by either then [`boxplot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.boxplot.html#matplotlib.axes.Axes.boxplot) or [`plot.box()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.box.html) methods. They are aliases.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We'll first drop the \"Particle Number\" parameter because it's a completely different measure and magnitude.\n",
    "df_pm_nopn = df_pm.drop(columns=['Particle Number'])\n",
    "df_pm_nopn.plot.box(ylabel='ug/m3');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We should probably plot the parameters on their own panel due the differences in magnitude."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pm_nopn.plot.box(subplots=True,  # Plot each column in on its own panel\n",
    "                    ylabel='ug/m3');\n",
    "plt.subplots_adjust(wspace=.5)  # Adjust space between subplots so they don't overlap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like there are outliers that may be worth removing in future analysis.\n",
    "Collecting more years of data could give us a better idea of these are true outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.2 Time Series\n",
    "Time series are useful to look at temporal trends.\n",
    "To plot a time series we simply call the method [`plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a time series\n",
    "df_pm_nopn.plot();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PM10 did not show up because it has a lot of NaNs. We can plot the data points to see this by providing keyword arguments to the plot command. Here we add many other keyword arguments.\n",
    "\n",
    "See the [`plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) documentation for other options."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = df_pm_nopn.plot(marker='o',  # Add circle markers\n",
    "                ylabel='ug/m3',  # Adds y-axis label\n",
    "                logy=True,  # Converts y-axis to a log-scal\n",
    "                alpha=.6,   # Transparency scale from 0-1, 0 being invisible\n",
    "                figsize=(12, 5));  # width and height of the figure\n",
    "\n",
    "# Add a dashed line for PM2.5 criteria value at 12 ug/m3\n",
    "ax.axhline(12, linestyle='--', color='gray');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Environmental data can be noisy. Applying a rolling mean is a technique to smooth-out noise and look at underlying trends.\n",
    "We use the [`rolling()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) method to the DataFrame to calculate this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pm_nopn_rm5 = df_pm_nopn.drop(columns=['PM10']).rolling(5).mean()  # Compute a 5-day rolling average\n",
    "df_pm_nopn_rm5.plot(marker='o',  # Add circle markers\n",
    "                    ylabel='ug/m3',  # Adds y-axis label\n",
    "                    alpha=.6,   # Transparency scale from 0-1, 0 being invisible\n",
    "                    subplots=True,\n",
    "                    figsize=(12, 5));"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* There's not enough data to compute a rolling average for PM10.\n",
    "* Black carbon and PM2.5 seem paired. This makes sense because Black carbon PM2.5 is a subset of PM2.5.\n",
    "* There seems to be some temporal trends."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.3 Scatter\n",
    "Scatter plots help determine relationship between two variables.\n",
    "Let's plot a scatter plot between PM2.5 and Black Carbon. We can specify this with the [`plot.scatter()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.scatter.html#pandas.DataFrame.plot.scatter) method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pm.plot.scatter(x='PM2.5', y='Black Carbon');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A [`scatter_matrix`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) function is a quick method to look at relationships between all parameters simultaneously.\n",
    "\n",
    "Note that this a function and not a method called on the DataFrame. We pass the DataFrame as an argument into the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "df_pm_renamed = df_pm.copy()\n",
    "scatter_matrix(df_pm, figsize=(6,6));"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the parameters are generally positively correlated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9.4 Autocorrelation & Lag Plots\n",
    "Autocorrelation and lag plots help us determine if there are relationship with a measurement and previous measurements (lags).\n",
    "Autocorrelation can find periodic trends in the data like seasonality.\n",
    "\n",
    "pandas [`autocorrelation_plot`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html) function computes the correlation across all the lags for a set of measurements."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot, lag_plot\n",
    "\n",
    "# We pass a single column into an autocorrelation plot function.\n",
    "# The column cannot have null values. We call the method `fillna` to backward-fill null values with the subsequent values. This isn't an issue because there are few nulls.\n",
    "autocorrelation_plot(df_pm_renamed['PM2.5'].fillna(method=\"bfill\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autocorrelation_plot(df_pm_renamed['PM2.5'].fillna(method=\"ffill\")).set_xlim(1,20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The horizontal lines correspond to the 99 (dashed) and 95 (solid) confidence bands. When the line is outside the bands the correlation is more confidently non-zero. The lags represent days. There is a steep drop-off in correlation after the first day.\n",
    "\n",
    "We can look at the scatter plots with specific lags with the [`lag_plot`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.lag_plot.html) function.\n",
    "\n",
    "Below we plot progressively larger lags. The plotting method below is an example of matplotlib syntax."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create figure with 4 subplots. The fig variable is the object for configuring the entire figure (all subplots) the axs variable is an array with 4 axis objects corresponding to each subplot.\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(6,6))\n",
    "axs = axs.ravel()  # Flatten the array\n",
    "# Plot each lag and specify which subpanel it should go to with the `ax` keyword\n",
    "lag_plot(df_pm_renamed['PM2.5'], lag=1, ax=axs[0]);\n",
    "lag_plot(df_pm_renamed['PM2.5'], lag=2, ax=axs[1]);\n",
    "lag_plot(df_pm_renamed['PM2.5'], lag=4, ax=axs[2]);\n",
    "lag_plot(df_pm_renamed['PM2.5'], lag=12, ax=axs[3]);\n",
    "plt.tight_layout()  # Adjust the subplot layout so they don't overlap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
