{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis: Hands on Data\n",
    "Today we will be demonstrating the following key exploratory data analysis techniques using an example dataset:\n",
    "**Agenda:**\n",
    "1. Importing libraries & packages\n",
    "2. Importing tabular data to a DataFrame\n",
    "3. Inspecting DataFrame structure\n",
    "4. Concatenation\n",
    "5. Renaming columns\n",
    "6. Exploring values\n",
    "7. Handling NaNs and Nulls\n",
    "8. Plotting\n",
    "\n",
    "\n",
    "\n",
    "## The Data\n",
    "Our example dataset is daily summaries of air quality data from Providence, RI. It will give you some experience with working with temporal data.\n",
    "\n",
    "The Rhode Island Department of Environmental Management (RIDEM) and Rhode Island Department of Health (RIDOH) collects air quality data at several sites across Rhode Island. We will be examining data from one site at the Community of Rhode Island (CCRI) Liston Campus. Here's some background:\n",
    "\n",
    "* The CCRI site is part of the EPA's *State or Local Air Monitoring Stations* (SLAMS) and *National Air Toxics Trends Sites* (NATTS) networks.\n",
    "* A variety of air pollutants (particulate matter (PM), volatile organic carbon (VOCs),  polycyclic aromatic hydrocarbons (PAHs), carbonyls, black carbon) have been monitored at this site since 2005.\n",
    "* A reference for some of the dataset's [field descriptions](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_daily_summary_files).\n",
    "* The data was obtained from the Environmental Protection Agency (EPA) [Air Quality Data website](https://www.epa.gov/outdoor-air-quality-data).\n",
    "        <div>\n",
    "        <img src=\"images/aq-site-info.png\" width=\"400\"/>\n",
    "        </div>\n",
    "\n",
    "We will use a subset of this data in the demonstrations below and give you a chance to work with a larger dataset during the hands-on lab.\n",
    "\n",
    "*Links*\n",
    "[EPA Air Quality Data Interactive Map](https://www.epa.gov/outdoor-air-quality-data/interactive-map-air-quality-monitors) - Data source\n",
    "[RIDEM 2022 Annual Monitoring Report](https://dem.ri.gov/sites/g/files/xkgbur861/files/2023-01/airnet22.pdf) - More information about the site and other monitoring locations across the state.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Importing libraries & packages\n",
    "Importing packages typically appears at the top of the file.\n",
    "* `import <package_name>` is the most basic command\n",
    "* The package can be imported with an alias to shorten verbosity. Common packages will often have a conventional alias.\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "pandas.read_csv(path)\n",
    "\n",
    "# VS as an alias\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_csv(path)\n",
    "```\n",
    "</blockquote>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas library as an alias of 'pd'\n",
    "import matplotlib.pyplot as plt  # Import the sub-package pyplot from the matplotlib library as an alias of 'plt'\n",
    "import os  # Import standard library operating system package that deals with system directory interfaces\n",
    "from pathlib import Path  # Import filesystem path package, for easier pathing to files and outputs\n",
    "\n",
    "# Magic command for jupyter notebook to generate figures within the notebook\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Importing tabular data to a DataFrame\n",
    "The pandas package reads tabular data into a data structure called a `DataFrame`. Some examples of read functions are below:\n",
    "* [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) - Comma-delimited or other delimited files\n",
    "* [`pd.read_fwf`](https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html#) - Fixed width files\n",
    "* [`pd.read_excel`](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) - Microsoft excel files\n",
    "* [`pd.read_sql`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) - SQL query or database table\n",
    "* See [pandas I/O documentation](https://pandas.pydata.org/docs/reference/io.html#input-output) for more examples\n",
    "\n",
    "We will be working with the `pd.read_csv()` because our data is comma-delimited. This function defaults to read comma-delimited files, but can be used on any delimited text file when the seperator is specified.\n",
    "\n",
    "A. To start we need specify the path to our data directory:\n",
    "```\n",
    "project\n",
    "├── data\n",
    "│   └── raw\n",
    "│       └── monthly            <- Data is here\n",
    "│\n",
    "└── notebooks                  <- Our working directory is here\n",
    "```\n",
    "We will be using package `os` and `Path` from `pathlib` to create out directory path because it standardizes pathing between operating systems. Path separators are different between Unix (Mac & Linux; using `/`) and Windows (using `\\`) operating systems. Avoiding full string paths makes the code universal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "project_top = Path('..', 'data', 'raw', 'monthly')\n",
    "project_top"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Create the\n",
    "project_top = Path('..', 'data', 'raw', 'monthly')\n",
    "\n",
    "## We get our current working directory\n",
    "cwd = os.getcwd()\n",
    "print(f'The current working directory is where this notebook is located: {cwd}')\n",
    "\n",
    "## We initialize a Path object named project_top using the cwd and define the top-level of the project with the parent directory\n",
    "project_top = Path(cwd).parents[0]\n",
    "print(f'This is the the top level of the project: {project_top}')\n",
    "\n",
    "## We extend the path to the monthly data directory\n",
    "path_to_monthly_data = project_top / 'data' / 'raw' / 'monthly'\n",
    "path_to_monthly_data2 = project_top.joinpath('data', 'raw',\n",
    "                                             'monthly')  # Alternative syntax for extending path\n",
    "\n",
    "print(f'This is the monthly data directory: {path_to_monthly_data}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the path generated above, we will read the first month of data (January 2022)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read and save the DataFrame object to a variable 'df_2022_01'\n",
    "df_2022_01 = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_01.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspecting DataFrame Structure\n",
    "Now that we have imported the data to a DataFrame. Some questions we are curious about:\n",
    "1. Did it import correctly?\n",
    "2. What does the table look like? Number of rows? Columns?\n",
    "3. Do we need all the data we are importing?\n",
    "4. Is the data in the correct format?\n",
    "\n",
    "We can inspect the DataFrame object by looking at its **attributes** and using DataFrame **methods**.\n",
    "\n",
    "Here are useful **attributes** of the dataframe\n",
    "* `.shape`:  Table dimensions\n",
    "* `.columns`:  Sequence of columns\n",
    "* `.index`:  Sequence of row indexes/labels\n",
    "* `.dtypes`: Data types by column\n",
    "\n",
    "Here are a few useful **methods** to inspect a dataframe:\n",
    "* `.head()`: Shows the first 5 rows--can change the number by supplying an integer.\n",
    "* `.tail()`: Shows the last 5 rows--can change the number by supplying an integer.\n",
    "* `.info()`: Combines several DataFrame attributes to one report.\n",
    "* `.select_dtypes()`: Useful for viewing only columns of certain data types.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Python objects may have <b>attributes</b> and <b>methods</b>.\n",
    "\n",
    "<b>attributes</b> - Are properties of the object type. Say that there is a `Person` object, the person's favorite food is one of their properties.\n",
    "<b>methods</b> - Are functions bound to an object type. They often perform a process that uses the object's properties. A method of a `Person` object, could be report writing.\n",
    "\n",
    "Attributes and methods are accessed by using dot (`.`) connectors to the object. The difference is that methods have `()` at the end so arguments can be passed.\n",
    ">Example:\n",
    "> ```Python\n",
    "> George.favorite_food  # Accessing an attribute\n",
    "> >>> 'Pho'\n",
    "> my_report = George.write_report(topic='favorite_food', pages=5)  # Calling a method\n",
    "> ```\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "What is an \"object\" dtype?\n",
    "\n",
    "<b>Short Answer:</b> It is a column of string or mixed data types (e.g. string, ints, floats, etc). Typically object dtype columns from an imported CSV will be a column of strings.\n",
    "\n",
    "<b>Long Answer:</b>  Pandas was built upon the numpy package on its backend. Numpy can only store information in an array where each value is encoded in the same number of bytes. Because strings can be of variable length, they do not conform to the fixed byte requirement. Instead Pandas creates an object array with pointers to the strings and  the pointers are of equal byte size. This is similar for columns with mixtures of data types.\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect Numerical Fields\n",
    "df_2022_01.select_dtypes(include=['int', 'float']).head(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect Object fields\n",
    "df_2022_01.select_dtypes(include='object').head(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Back to our questions:**\n",
    "\n",
    "1. Did it import correctly?\n",
    "2. What does the table look like? Number of rows? Columns?\n",
    "3. Do we need all the data we are importing?\n",
    "4. Is the data in the correct format?\n",
    "\n",
    "* There are many columns we could drop because they all have the same value such as: \"Local Site Name\" and \"Address\". We know we are only working with one site for this analysis so these columns don't provide much value. These columns are long string fields that take up more memory. Dropping them would improve performance if this dataset gets really large.\n",
    "* The date would be more useful as a datetime data type rather than as string. This will allow for filtering by time and other useful datetime operations.\n",
    "\n",
    "We can supply additional arguments to the `read_csv` function to handle these specifications."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of the columns we wish to keep\n",
    "keep_cols = ['Parameter Code', 'POC', 'Parameter Name', 'Duration Description',\n",
    "             'Pollutant Standard',\n",
    "             'Date (Local)', 'Year', 'Day In Year (Local)', 'Units of Measure',\n",
    "             'Exceptional Data Type',\n",
    "             'Observation Count', 'Observation Percent', 'Arithmetic Mean', 'First Maximum Value',\n",
    "             'First Maximum Hour', 'AQI', 'Daily Criteria Indicator', ]\n",
    "\n",
    "# Read in the csv with additional arguments\n",
    "df_2022_01_curated = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_01.csv',\n",
    "                                 usecols=keep_cols,  # Specify columns to keep\n",
    "                                 parse_dates=['Date (Local)'],  # Specify column to parse as a date instead of string\n",
    "                                 date_format='%Y-%m-%d',  # Specify the format of date strings\n",
    "                                 )\n",
    "df_2022_01_curated.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01_curated.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! We've cut down the number of columns and converted the date field to a datetime format!\n",
    "Next lets see how we can add more data from other files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Concatenation\n",
    "So far we've worked with one month's worth of data. Let's see how we can combine DataFrames together.\n",
    "\n",
    "We will be using the [`pd.concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) function to combine two or more DataFrames.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read in February data\n",
    "df_2022_02_curated = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_02.csv',\n",
    "                                 usecols=keep_cols,  # Specify columns to keep\n",
    "                                 parse_dates=['Date (Local)'],  # Specify column to parse as a date instead of string\n",
    "                                 date_format='%Y-%m-%d',  # Specify the format of date strings\n",
    "                                 )\n",
    "list_df_to_concat = [df_2022_01_curated, df_2022_02_curated]\n",
    "df_combined = pd.concat(list_df_to_concat)\n",
    "df_combined.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_combined.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_01_curated.query(\"`Parameter Code` == 88101\").filter(\n",
    "    ['Duration Description', 'Date (Local)', 'Arithmetic Mean', 'First Maximum Value',\n",
    "     'First Maximum Hour'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the online documentation for this function. [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "\n",
    " At the top is the function call signature:\n",
    ">pandas.read_csv(filepath_or_buffer, *, sep=_NoDefault.no_default, delimiter=None, header='infer', ...)\n",
    "* This demonstrates how to use the function in code with all the available arguments.\n",
    "* There are two types of arguments: *Positional* and *Keyword*\n",
    "    1. **Positional arguments** are listed first. They are required and need to be specified in order. In this example there is only one, `filepath_or_buffer`.\n",
    "    2. **Keyword arguments** are listed after positional arguments and are optional. They have an `=` after the name to denote default values.\n",
    "\n",
    "    Positional arguments do not need to be specified by name while keyword arguments must be specified by name.\n",
    "    ```python\n",
    "    # Both of these calls are acceptable\n",
    "    pd.read_csv('data/raw/datafile.csv', sep=',')\n",
    "    pd.read_csv(filepath_or_buffer='data/raw/datafile.csv', sep=',')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
