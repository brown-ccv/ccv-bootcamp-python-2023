{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Hands on Data\n",
    "Today we will be demonstrating the following key exploratory data analysis techniques using an example dataset:\n",
    "\n",
    "**Agenda:**\n",
    "1. Importing libraries & packages\n",
    "2. Importing tabular data to a DataFrame\n",
    "3. Inspecting DataFrame structure\n",
    "4. Concatenation\n",
    "5. Renaming Columns\n",
    "6. Exploring values\n",
    "7. Exporting DataFrames\n",
    "8. Merging\n",
    "9. Plotting\n",
    "\n",
    "\n",
    "\n",
    "## The Data\n",
    "Our example dataset is daily summaries of air quality data from Providence, RI. It will give you some experience with working with temporal data.\n",
    "\n",
    "The Rhode Island Department of Environmental Management (RIDEM) and Rhode Island Department of Health (RIDOH) collects air quality data at several sites across Rhode Island. We will be examining data from one site at the Community of Rhode Island (CCRI) Liston Campus. Here's some background:\n",
    "\n",
    "* The CCRI site is part of the EPA's *State or Local Air Monitoring Stations* (SLAMS) and *National Air Toxics Trends Sites* (NATTS) networks.\n",
    "* A variety of air pollutants (particulate matter (PM), volatile organic carbon (VOCs),  polycyclic aromatic hydrocarbons (PAHs), carbonyls, black carbon) have been monitored at this site since 2005.\n",
    "* A reference for some of the dataset's [field descriptions](https://aqs.epa.gov/aqsweb/airdata/FileFormats.html#_daily_summary_files).\n",
    "* The data was obtained from the Environmental Protection Agency (EPA) [Air Quality Data website](https://www.epa.gov/outdoor-air-quality-data).\n",
    "        <div>\n",
    "        <img src=\"./images/aq-site-info.png\" width=\"400\"/>\n",
    "        </div>\n",
    "\n",
    "We will use a subset of this data in the demonstrations below and give you a chance to work with a larger dataset during the hands-on lab.\n",
    "\n",
    "*Links*\n",
    "[EPA Air Quality Data Interactive Map](https://www.epa.gov/outdoor-air-quality-data/interactive-map-air-quality-monitors) - Data source\n",
    "[RIDEM 2022 Annual Monitoring Report](https://dem.ri.gov/sites/g/files/xkgbur861/files/2023-01/airnet22.pdf) - More information about the site and other monitoring locations across the state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries & packages\n",
    "Importing packages typically appears at the top of the file.\n",
    "* `import <package_name>` is the most basic command\n",
    "* The package can be imported with an alias to shorten verbosity. Common packages will often have a conventional alias.\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "pandas.read_csv('path')\n",
    "\n",
    "# VS as an alias\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_csv('path')\n",
    "```\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Import pandas library as an alias of 'pd'\n",
    "# import matplotlib.pyplot as plt  # Import the sub-package pyplot from the matplotlib library as an alias of 'plt'\n",
    "from pathlib import Path  # Import filesystem path package, for easier pathing to files and outputs\n",
    "\n",
    "# Magic command for jupyter notebook to generate figures within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing tabular data to a DataFrame\n",
    "The pandas package reads tabular data into a data structure called a `DataFrame`. Some examples of read functions are below:\n",
    "* [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) - Comma-delimited or other delimited files\n",
    "* [`pd.read_fwf`](https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html#) - Fixed width files\n",
    "* [`pd.read_excel`](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) - Microsoft excel files\n",
    "* [`pd.read_sql`](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html) - SQL query or database table\n",
    "* See [pandas I/O documentation](https://pandas.pydata.org/docs/reference/io.html#input-output) for more examples\n",
    "\n",
    "We will be working with the `pd.read_csv()` because our data is comma-delimited. This function defaults to read comma-delimited files, but can be used on any delimited text file when the seperator is specified.\n",
    "\n",
    "A. To start we need specify the path to our data directory:\n",
    "```\n",
    "project\n",
    "├── data\n",
    "│   └── raw\n",
    "│       └── monthly   <- Data is here\n",
    "│\n",
    "└── notebooks         <- Our working directory is here\n",
    "```\n",
    "We will be using package `os` and `Path` from `pathlib` to create out directory path because it standardizes pathing between operating systems. Path separators are different between Unix (Mac & Linux; using `/`) and Windows (using `\\`) operating systems. Avoiding full string paths makes the code universal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to the data directory. The `..` means go up one level from the current working directory\n",
    "data_path = Path('..', 'data')\n",
    "\n",
    "## We extend the path to the monthly data directory\n",
    "path_to_monthly_data = data_path / 'raw' / 'monthly'\n",
    "path_to_monthly_data2 = data_path.joinpath('raw',\n",
    "                                           'monthly')  # Alternative syntax for extending path\n",
    "\n",
    "print(f'This is the monthly data directory: {path_to_monthly_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the path generated above, we will read the first month of data (January 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and save the DataFrame object to a variable 'df_2022_01'\n",
    "df_2022_01 = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting DataFrame Structure\n",
    "Now that we have imported the data to a DataFrame. Some questions we are curious about:\n",
    "1. Did it import correctly?\n",
    "2. What does the table look like? Number of rows? Columns?\n",
    "3. Do we need all the data we are importing?\n",
    "4. Is the data in the correct format?\n",
    "\n",
    "We can inspect the DataFrame object by looking at its **attributes** and using DataFrame **methods**.\n",
    "\n",
    "Here are useful **attributes** of the dataframe\n",
    "* `.shape`:  Table dimensions\n",
    "* `.columns`:  Sequence of columns\n",
    "* `.index`:  Sequence of row indexes/labels\n",
    "* `.dtypes`: Data types by column\n",
    "\n",
    "Here are a few useful **methods** to inspect a dataframe:\n",
    "* `.head()`: Shows the first 5 rows--can change the number by supplying an integer.\n",
    "* `.tail()`: Shows the last 5 rows--can change the number by supplying an integer.\n",
    "* `.info()`: Combines several DataFrame attributes to one report.\n",
    "* `.select_dtypes()`: Useful for viewing only columns of certain data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Python objects may have <b>attributes</b> and <b>methods</b>.\n",
    "\n",
    "<b>attributes</b> - Are properties of the object type. Say that there is a `Person` object, the person's favorite food is one of their properties.\n",
    "<b>methods</b> - Are functions bound to an object type. They often perform a process that uses the object's properties. A method of a `Person` object, could be report writing.\n",
    "\n",
    "Attributes and methods are accessed by using dot (`.`) connectors to the object. The difference is that methods have `()` at the end so arguments can be passed.\n",
    ">Example:\n",
    "```\n",
    "George.favorite_food  # Accessing an attribute\n",
    ">>> 'Pho'\n",
    "my_report = George.write_report(topic='favorite_food', pages=5)  # Calling a method\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "What is an \"object\" dtype?\n",
    "\n",
    "<b>Short Answer:</b> It is a column of string or mixed data types (e.g. string, ints, floats, etc). Typically object dtype columns from an imported CSV will be a column of strings.\n",
    "\n",
    "<b>Long Answer:</b>  Pandas was built upon the numpy package on its backend. Numpy can only store information in an array where each value is encoded in the same number of bytes. Because strings can be of variable length, they do not conform to the fixed byte requirement. Instead Pandas creates an object array with pointers to the strings and  the pointers are of equal byte size. This is similar for columns with mixtures of data types.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Numerical Fields\n",
    "df_2022_01.select_dtypes(include=['int', 'float']).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Object fields\n",
    "df_2022_01.select_dtypes(include='object').head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to our questions:**\n",
    "\n",
    "1. Did it import correctly?\n",
    "2. What does the table look like? Number of rows? Columns?\n",
    "3. Do we need all the data we are importing?\n",
    "4. Is the data in the correct format?\n",
    "\n",
    "* There are many columns we could drop because they all have the same value such as: \"Local Site Name\" and \"Address\". We know we are only working with one site for this analysis so these columns don't provide much value. These columns are long string fields that take up more memory. Dropping them would improve performance if this dataset gets really large.\n",
    "* The date would be more useful as a datetime data type rather than as string. This will allow for filtering by time and other useful datetime operations.\n",
    "\n",
    "We can supply additional arguments to the `read_csv` function to handle these specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the columns we wish to keep\n",
    "keep_cols = ['Parameter Code', 'POC', 'Parameter Name', 'Duration Description',\n",
    "             'Pollutant Standard',\n",
    "             'Date (Local)', 'Year', 'Day In Year (Local)', 'Units of Measure',\n",
    "             'Exceptional Data Type',\n",
    "             'Observation Count', 'Observation Percent', 'Arithmetic Mean', 'First Maximum Value',\n",
    "             'First Maximum Hour', 'AQI', 'Daily Criteria Indicator', ]\n",
    "\n",
    "# Read in the csv with additional arguments\n",
    "df_2022_01_curated = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_01.csv',\n",
    "                                 usecols=keep_cols,  # Specify columns to keep\n",
    "                                 parse_dates=['Date (Local)'],\n",
    "                                 # Specify column to parse as a date instead of string\n",
    "                                 date_format='%Y-%m-%d',  # Specify the format of date strings\n",
    "                                 )\n",
    "df_2022_01_curated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_01_curated.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've cut down the number of columns and converted the date field to a datetime format!\n",
    "Next lets see how we can add more data from other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concatenation\n",
    "So far we've worked with one month's worth of data. Let's see how we can combine DataFrames together.\n",
    "\n",
    "We will be using the [`pd.concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) function to combine two or more DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in February data\n",
    "df_2022_02_curated = pd.read_csv(path_to_monthly_data / 'daily_44_007_0022_2022_02.csv',\n",
    "                                 usecols=keep_cols,  # Specify columns to keep\n",
    "                                 parse_dates=['Date (Local)'],\n",
    "                                 # Specify column to parse as a date instead of string\n",
    "                                 date_format='%Y-%m-%d',  # Specify the format of date strings\n",
    "                                 )\n",
    "list_df_to_concat = [df_2022_01_curated, df_2022_02_curated]\n",
    "df_combined = pd.concat(list_df_to_concat)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "After we concat we need to remember to reset the index because it does not do this automatically! We use the method `reset_index()` to do this.\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Notice how the index is not the length of the dataframe?\n",
    "df_combined.index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Resetting index\n",
    "df_combined = df_combined.reset_index(drop=True)\n",
    "df_combined.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat all the files!\n",
    "Now that we've learned how to concatenate files. Let's combine all the monthly data.\n",
    "Doing it manually for each file would be cumbersome. So lets use a function!\n",
    "\n",
    "The function is not coded in this notebook but rather contained in the source code (src) directory. Why?\n",
    "\n",
    "* This function seems like it could be useful in other analyses and scripts. Saving it in a common utility location gives us easy access in future analysis.\n",
    "\n",
    "\n",
    "We've written this script already. It lives in the file at the directory path listed below.\n",
    "```\n",
    "project\n",
    "├── src\n",
    "│   └── data\n",
    "│       └── utils.py   <- Function is saved here\n",
    "```\n",
    "We encourage you to take a look because it is a good example of writing a function covering with concepts of: 1) listing files in a directory, 2) for loops, and 3) if/else constructs combined with what we just learned about reading csv and concatenation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "If we have not already, we need to install our src directory as a package to our environment to be able to access it.\n",
    "\n",
    "`%pip install -e ..`\n",
    "\n",
    "If we do it now we'll need to restart our kernel and run all cells to here before we continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function from our source code directory\n",
    "from src.data.utils import combine_csv_files\n",
    "\n",
    "df_2022 = combine_csv_files(path_to_monthly_data,\n",
    "                            prefix='daily_44_007_0022_2022_',\n",
    "                            # Read in files only starting with this prefix\n",
    "                            suffix='.csv',  # Read in files only ending with this suffix\n",
    "                            usecols=keep_cols,  # Specify columns to keep\n",
    "                            parse_dates=['Date (Local)'],\n",
    "                            # Specify column to parse as a date instead of string\n",
    "                            date_format='%Y-%m-%d', )  # Specify the format of date strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Renaming Columns\n",
    "Before we jump into more detailed EDA we'll want to update the DataFrame to make our lives a bit easier. We'll be typing column names often to query the data so lets start by renaming the columns a standard format.\n",
    "\n",
    "1. lowercase\n",
    "2. underscores instead of spaces\n",
    "3. simplify complex names\n",
    "\n",
    "Methods we'll use:\n",
    "1. [`str.lower()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html): Lowercases the column names\n",
    "2. [`str.replace()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html): Finds and replaces sub-strings\n",
    "3. [`rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html): Renames columns or index labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we access columns with the columns attribute\n",
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can cast a lowercase method across the columns with the `str.lower()` method\n",
    "df_2022.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cast a `str.replace()` method after the lowercase method. Though this starts to look hard to read.\n",
    "df_2022.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# A cleaner way is to wrap it in (), allowing us to separate the dot connectors to multiple lines\n",
    "(df_2022.columns\n",
    " .str.lower()  # Lowercase the names\n",
    " .str.replace(' ', '_')  # Replace Spaces with underscores\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above examples are output demonstrations. It doesn't actually change the dataframe's column until we assign the output.\n",
    "print('---Before Assignment---')\n",
    "print(df_2022.columns)\n",
    "\n",
    "# Assigning reformatted column names to the DataFrame's columns attribute\n",
    "df_2022.columns = (df_2022.columns\n",
    "                   .str.lower()  # Lowercase the names\n",
    "                   .str.replace(' ', '_')  # Replace Spaces with underscores\n",
    "                   )\n",
    "print('---After Assignment---')\n",
    "print(df_2022.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the rename method to rename specific columns\n",
    "df_2022 = df_2022.rename(columns={'date_(local)': 'date',\n",
    "                                  'day_in_year_(local)': 'day_in_year',\n",
    "                                  }\n",
    "                         )\n",
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! Now are column names are much more manageable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Exploring Values\n",
    "Let's start exploring the dataset.\n",
    "\n",
    "Questions:\n",
    "1. How complete is the data?\n",
    "2. How many parameters are measured?\n",
    "3. How often are the parameters measured?\n",
    "4. What are the descriptive stats of the numeral data?\n",
    "\n",
    "We will cover the following topics:\n",
    "1. Checking for Nulls\n",
    "2. Indexing\n",
    "3. Counts and Uniques\n",
    "4. Querying\n",
    "5. Descriptive Stats and groupby"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1 Checking for Nulls\n",
    "Nulls or NAs are values that represent no data or missing data.\n",
    "* They are often represented as **NaN** for \"not a number\"\n",
    "* Pandas will typically fill NaNs for blank values upon import.\n",
    "* Be aware that scientific datasets often use large negative numbers outside of the normal range (like -999) to represent null data.\n",
    "\n",
    "We can check for the number of null values quickly using the method `isna()` and summing the results\n",
    "* `isna()` will create a boolean matrix and `sum()` will sum by columns.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luckily this dataset does not have missing data in important fields. The fields with null values make sense.\n",
    "* Not all chemicals measured have a regulatory standard that would be in the *pollutant_standard* field, only the most toxic.\n",
    "* *exceptional_data_type* is a flag field for anomalous conditions and events.\n",
    "\n",
    "Pandas has great documentation for working with and filling in missing data that we recommend reviewing. [Link](https://pandas.pydata.org/docs/user_guide/missing_data.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2 Indexing\n",
    "We index a dataframe using brackets.\n",
    "\n",
    "```\n",
    "# For a single column\n",
    "df_2022['parameter_name']\n",
    "\n",
    "# For multiple columns\n",
    "df_2022[['parameter_name', 'parameter_code']]\n",
    "\n",
    "# For a slice of rows (same as list indexing)\n",
    "df_2022[5:10]\n",
    "\n",
    "# For a slice of rows and columns use the .loc method\n",
    "df_2022.loc[5:10, ['parameter_name', 'parameter_code']]\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022.loc[5:10, ['parameter_name', 'parameter_code']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.3 Counts and Uniques\n",
    "Counts and unique values are great for looking at categorical data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the count of unique values in each column\n",
    "df_2022.nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a list of unique values in a column\n",
    "df_2022['parameter_name'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the counts of each parameter\n",
    "counts = df_2022['parameter_name'].value_counts()\n",
    "counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is abbreviated. To see specific analytes you need index with a list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts[['PM2.5 - Local Conditions', 'Lead PM10 STP', 'Acetaldehyde', 'Cyclohexane']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of Particulate Matter <2.5 microns (PM2.5) measurements seems high. Let's take a closer look at this parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.4 Querying\n",
    "There are two main ways to query a DataFrame:\n",
    "1. Masking and Indexing\n",
    "2. [`query()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html) method\n",
    "\n",
    "The choice depends on preference and context.\n",
    "* Masking and Indxing is clearer for complex queries with multiple conditions.\n",
    "* The `query()` method good for short and quick queries.\n",
    "\n",
    "You can try either method below by comment/uncommenting lines 3 and 6."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Masking and Indexing\n",
    "mask_param = df_2022['parameter_name'] == 'PM2.5 - Local Conditions'\n",
    "df_2022[mask_param].head(12)\n",
    "\n",
    "# Query method\n",
    "# df_2022.query('parameter_name == \"PM2.5 - Local Conditions\"').head(12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022.loc[mask_param, ['parameter_name', 'duration_description', 'pollutant_standard', 'arithmetic_mean', 'first_maximum_value', 'first_maximum_hour','aqi']].head(12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*What do we notice about the PM2.5 measurement?*\n",
    "* There are 3 records per day. They differ in duration_description and pollutant_standard.\n",
    "* The \"1 HOUR\" duration seems to have the most accurate information for our purposes because has more \"arithmetic_mean\" precision, and better information about the first maximum value and hour over the day.\n",
    "* The \"1 Hour\" duration records does not have Air Quality Index \"aqi\" information\n",
    "\n",
    "**Conclusion**\n",
    "* We should keep only the \"1 HOUR\" duration and remove the others.\n",
    "* We'll save the other durations in a separate DataFrame in the case we want the daily AQI information.\n",
    "\n",
    "We can do this with querying!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a mask for the values we want to remove\n",
    "mask_remove = (df_2022['parameter_name'] == 'PM2.5 - Local Conditions') & (df_2022['duration_description'] != '1 HOUR')\n",
    "\n",
    "# Create dataframe of PM2.5 records we removed\n",
    "df_removed_records = df_2022[mask_remove].reset_index(drop=True)\n",
    "\n",
    "# Create dataframe of everything we want to keep. The NOT (~) operator reverses the mask.\n",
    "df_2022_cleaned_v1 = df_2022[~mask_remove].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts2 = df_2022_cleaned_v1['parameter_name'].value_counts()\n",
    "counts2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts2['PM2.5 - Local Conditions']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.5 Descriptive Stats and groupby\n",
    "The method `.describe()` is a great way to get descriptive statistics on numerical columns. When called on the dataframe, it will automatically provide statistics on each numerical column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_cleaned_v1.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Though this isn't very useful for our dataset at the moment because it's creating statistics across 90+ parameters. It would be better to chunk the DataFrame by parameter and run describe. This is where a powerful method called `groupby` is useful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a Groupby object with parameter_name\n",
    "gb_param = df_2022_cleaned_v1.groupby(by='parameter_name')\n",
    "gb_param['arithmetic_mean'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To look at the PM2.5 stats\n",
    "df_gb_stats = gb_param['arithmetic_mean'].describe()\n",
    "df_gb_stats.loc['PM2.5 - Local Conditions']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can groupby multiple categories. Here's an example of sample counts by parameter and month."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gb_param_month = df_2022_cleaned_v1.groupby(by=['parameter_name', df_2022_cleaned_v1['date'].dt.month])\n",
    "df_param_monthly_counts = gb_param_month['arithmetic_mean'].count()\n",
    "df_param_monthly_counts.loc[['PM2.5 - Local Conditions', '1,1,1,2-Tetrachloroethane']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Exporting DataFrames\n",
    "We made some interesting groupby tables with summary stats in the last section. Let's try exporting them to view in another application.\n",
    "\n",
    "We use the `to_csv()` method to export to csv.\n",
    "See the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) to see all the other formats you can export to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a Path object to the reports folder\n",
    "outputs_path = Path('..', 'reports')\n",
    "\n",
    "# Save the groupby parameters and descriptive stats of the arithmetic mean to a DataFrame\n",
    "df_gb_param_mean_describe = gb_param['arithmetic_mean'].describe()\n",
    "\n",
    "# Call the to_csv method on the dataframe specifying path and filename\n",
    "df_gb_param_mean_describe.to_csv(outputs_path / 'stats-arithmetic_mean-by-param.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Merging\n",
    "If we wanted to join two tables based on common key we would use the [`merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) method.\n",
    "\n",
    "There is a file with parameter classes that categorizes the parameters. This might help us make sense of which parameters are related to each other. Lets join it to our dataframe!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_param_classes = pd.read_csv(data_path / 'raw' / 'params_class.csv')\n",
    "df_param_classes.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count of the number of parameters in each class\n",
    "df_param_classes['class'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge the dataframes using parameter_code as the key with a left-join\n",
    "df_2022_cleaned_v2 = df_2022_cleaned_v1.merge(df_param_classes, on='parameter_code', how='left')\n",
    "df_2022_cleaned_v2.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We used a left-join here. This means that only keys from the left table matter in the join. Keys that are in the right table but not the left will not be joined.\n",
    "\n",
    "Notice that parameter_name is duplicated with an appended \"_x\" and \"_y\" because the column existed in both tables. You can prevent this by indexing only the columns you want to join. For now we are just going to drop and rename the columns for an example of the `drop()` method!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_2022_cleaned_v3 = (df_2022_cleaned_v2\n",
    "                      .drop(columns='parameter_name_y')  # Remove the column\n",
    "                      .rename(columns={'parameter_name_x': 'parameter_name'})  # We've seen this method previously!\n",
    "                      )\n",
    "df_2022_cleaned_v3.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the online documentation for this function. [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "\n",
    " At the top is the function call signature:\n",
    ">pandas.read_csv(filepath_or_buffer, *, sep=_NoDefault.no_default, delimiter=None, header='infer', ...)\n",
    "* This demonstrates how to use the function in code with all the available arguments.\n",
    "* There are two types of arguments: *Positional* and *Keyword*\n",
    "    1. **Positional arguments** are listed first. They are required and need to be specified in order. In this example there is only one, `filepath_or_buffer`.\n",
    "    2. **Keyword arguments** are listed after positional arguments and are optional. They have an `=` after the name to denote default values.\n",
    "\n",
    "    Positional arguments do not need to be specified by name while keyword arguments must be specified by name.\n",
    "    ```python\n",
    "    # Both of these calls are acceptable\n",
    "    pd.read_csv('data/raw/datafile.csv', sep=',')\n",
    "    pd.read_csv(filepath_or_buffer='data/raw/datafile.csv', sep=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
